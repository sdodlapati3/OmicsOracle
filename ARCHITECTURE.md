# ğŸ—ï¸ OmicsOracle Architecture Overview

## ğŸ¯ System Purpose
OmicsOracle is a genomics data analysis platform that processes natural language queries to retrieve and summarize relevant research data from NCBI GEO (Gene Expression Omnibus).

---

## ğŸ”„ Core Query Processing Flow

```mermaid
graph TD
    A[User Query] --> B[Web Interface]
    B --> C[Enhanced Query Handler]
    C --> D[Prompt Interpreter]
    D --> E[Advanced Search Enhancer]
    E --> F[Pipeline Orchestrator]
    F --> G[GEO Client]
    F --> H[AI Summary Manager]
    G --> I[NCBI GEO Database]
    H --> J[OpenAI API]
    I --> K[Raw GEO Data]
    J --> L[AI Summary]
    K --> M[Summarizer]
    L --> N[Final Response]
    M --> N
    N --> O[Web Response]
```

---

## ğŸ“ Core Architecture Components

### **ğŸŒ Presentation Layer**
```
src/omics_oracle/presentation/web/
â”œâ”€â”€ main.py              # FastAPI application entry point
â”œâ”€â”€ dependencies.py      # Dependency injection setup
â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ query.py        # Main query endpoint (/query)
â”‚   â”œâ”€â”€ health.py       # Health check endpoints
â”‚   â””â”€â”€ summary.py      # Summary endpoints
â”œâ”€â”€ middleware/
â”‚   â”œâ”€â”€ cors.py         # CORS configuration
â”‚   â”œâ”€â”€ logging.py      # Request/response logging
â”‚   â””â”€â”€ error_handler.py # Global error handling
â””â”€â”€ websockets.py       # Real-time communication
```

### **ğŸ” Search & Query Processing**
```
src/omics_oracle/search/
â”œâ”€â”€ enhanced_query_handler.py    # Main query coordinator
â””â”€â”€ advanced_search_enhancer.py  # Query optimization & enhancement
```

### **ğŸ§  Natural Language Processing**
```
src/omics_oracle/nlp/
â”œâ”€â”€ prompt_interpreter.py       # Query intent understanding
â””â”€â”€ biomedical_ner.py          # Biomedical entity recognition
```

### **âš™ï¸ Processing Pipeline**
```
src/omics_oracle/pipeline/
â””â”€â”€ pipeline.py                 # Main orchestration pipeline
```

### **ğŸ”— External Data Integration**
```
src/omics_oracle/geo_tools/
â””â”€â”€ geo_client.py               # NCBI GEO API client
```

### **ğŸ¤– AI Services**
```
src/omics_oracle/services/
â”œâ”€â”€ ai_summary_manager.py       # OpenAI integration
â”œâ”€â”€ summarizer.py              # Data summarization
â”œâ”€â”€ cost_manager.py            # API cost tracking
â””â”€â”€ cache.py                   # System-level caching (non-user-facing)
```

### **ğŸ› ï¸ Core Infrastructure**
```
src/omics_oracle/core/
â”œâ”€â”€ config.py                  # Configuration management
â”œâ”€â”€ models.py                  # Data models & schemas
â”œâ”€â”€ logging.py                 # Logging configuration
â””â”€â”€ exceptions.py              # Custom exceptions
```

---

## ğŸ”„ Detailed Query Processing Flow

### **1. Query Reception** ğŸ“¨
- **Entry Point**: `POST /query` endpoint in `routes/query.py`
- **Input**: Natural language query from user
- **Output**: Query object with metadata

### **2. Query Enhancement** ğŸš€
- **Component**: `enhanced_query_handler.py`
- **Process**:
  - Validates and preprocesses query
  - Coordinates with other components
  - Manages query lifecycle
- **Output**: Enhanced query object

### **3. Intent Understanding** ğŸ§ 
- **Component**: `prompt_interpreter.py`
- **Process**:
  - Analyzes query intent and context
  - Extracts biomedical entities
  - Determines search strategy
- **Output**: Structured query parameters

### **4. Search Optimization** ğŸ”
- **Component**: `advanced_search_enhancer.py`
- **Process**:
  - Refines search terms
  - Applies domain-specific knowledge
  - Optimizes for GEO database structure
- **Output**: Optimized search parameters

### **5. Pipeline Orchestration** âš™ï¸
- **Component**: `pipeline.py`
- **Process**:
  - Coordinates data retrieval and processing
  - Manages parallel operations
  - Handles error recovery
- **Output**: Orchestrated data flow

### **6. Data Retrieval** ğŸ“Š
- **Component**: `geo_client.py`
- **Process**:
  - Connects to NCBI GEO API
  - Retrieves relevant datasets
  - Handles API rate limiting
- **Output**: Raw GEO dataset information

### **7. AI Summarization** ğŸ¤–
- **Components**: `ai_summary_manager.py` + `summarizer.py`
- **Process**:
  - Sends data to OpenAI API
  - Generates human-readable summaries
  - Manages API costs and usage
- **Output**: Structured summaries

### **8. Response Assembly** ğŸ“‹
- **Component**: Query handler coordination
- **Process**:
  - Combines data and summaries
  - Formats for web response
  - Adds metadata and timing
- **Output**: Final JSON response

---

## ğŸ”§ Key Design Principles

### **1. Direct Data Flow** ğŸ¯
- No user-facing caching - all results are fresh from source
- Linear processing pipeline for predictability
- Clear separation of concerns

### **2. Fail-Safe Architecture** ğŸ›¡ï¸
- Graceful degradation when external APIs fail
- Comprehensive error handling and logging
- Timeout protection for all external calls

### **3. Scalable Design** ğŸ“ˆ
- Stateless components for horizontal scaling
- Async/await patterns for concurrent processing
- Configurable rate limiting and resource management

### **4. Maintainable Code** ğŸ§¹
- Single responsibility principle
- Clear dependency injection
- Comprehensive logging and monitoring

---

## ğŸŒ External Dependencies

### **Required Services**
- **NCBI GEO API**: Primary data source for genomics datasets
- **OpenAI API**: AI-powered summarization and analysis
- **FastAPI**: Web framework for REST API

### **Configuration**
- Environment-based configuration (dev/test/prod)
- API keys managed via environment variables
- Docker support for containerized deployment

---

## ğŸš€ Getting Started

### **Quick Start**
```bash
# Install dependencies
pip install -r requirements.txt

# Set environment variables
cp .env.example .env
# Edit .env with your API keys

# Start the application
./start.sh
```

### **Development**
```bash
# Start with development features
./start.sh --dev

# Backend only
./start.sh --backend-only

# Run tests
pytest tests/
```

---

## ğŸ“Š Performance Characteristics

- **Query Response Time**: ~2-10 seconds (depending on data complexity)
- **Concurrent Users**: Scales with container resources
- **API Rate Limits**: Managed automatically with backoff strategies
- **Memory Usage**: ~100-500MB per instance
- **Storage**: Minimal (no persistent user data caching)

---

**ğŸ” For detailed implementation information, see the source code in `src/omics_oracle/`**
