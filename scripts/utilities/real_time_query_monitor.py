#!/usr/bin/env python3
"""
Real-time Query Monitoring for OmicsOracle Futuristic Interface

This script monitors the backend in real-time to track user queries from the browser
and provides detailed analysis of the entire process flow.
"""

import asyncio
import aiohttp
import json
import time
import logging
from datetime import datetime
from typing import Dict, List, Any
import websockets
import threading

# Configure detailed logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('real_time_monitoring.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class RealTimeQueryMonitor:
    def __init__(self):
        self.monitoring = True
        self.query_sessions = {}
        self.last_query_time = 0
        
    async def monitor_api_requests(self):
        """Monitor API requests in real-time"""
        logger.info("üîç Starting real-time API monitoring...")
        logger.info("üì± Open your browser to http://localhost:8001 and submit a query!")
        logger.info("=" * 60)
        
        # Keep polling the backend to detect new queries
        while self.monitoring:
            try:
                async with aiohttp.ClientSession() as session:
                    # Check for new queries by monitoring the health endpoint
                    # and looking for changes in system state
                    await asyncio.sleep(0.5)  # Poll every 500ms
                    
            except Exception as e:
                logger.error(f"‚ùå Monitoring error: {e}")
                await asyncio.sleep(1)
    
    async def track_query_execution(self, query: str):
        """Track the execution of a specific query in detail"""
        start_time = time.time()
        query_id = f"query_{int(start_time)}"
        
        logger.info(f"üöÄ QUERY DETECTED: '{query}'")
        logger.info(f"üìã Query ID: {query_id}")
        logger.info(f"‚è∞ Start Time: {datetime.now()}")
        logger.info("=" * 60)
        
        # Track the query through the backend
        async with aiohttp.ClientSession() as session:
            try:
                # Step 1: Submit the query
                logger.info("üì§ STEP 1: Submitting query to backend API...")
                search_payload = {
                    "query": query,
                    "max_results": 10,
                    "search_type": "comprehensive"
                }
                
                api_start = time.time()
                async with session.post(
                    "http://localhost:8001/api/search",
                    json=search_payload,
                    headers={"Content-Type": "application/json"}
                ) as response:
                    api_end = time.time()
                    api_time = api_end - api_start
                    
                    logger.info(f"‚ö° API Response Time: {api_time:.3f}s")
                    logger.info(f"üìä HTTP Status: {response.status}")
                    
                    if response.status == 200:
                        result_data = await response.json()
                        await self.analyze_query_results(query_id, query, result_data, api_time)
                    else:
                        error_text = await response.text()
                        logger.error(f"‚ùå API Error: {response.status} - {error_text}")
                        
            except Exception as e:
                logger.error(f"üí• Query tracking failed: {e}")
    
    async def analyze_query_results(self, query_id: str, query: str, data: Dict[str, Any], api_time: float):
        """Analyze the query results in detail"""
        logger.info("üî¨ STEP 2: Analyzing query results...")
        
        # Basic metrics
        total_results = data.get("total_found", 0)
        search_time = data.get("search_time", 0)
        results = data.get("results", [])
        
        logger.info(f"üìà Results Summary:")
        logger.info(f"   ‚Ä¢ Total Found: {total_results}")
        logger.info(f"   ‚Ä¢ Backend Search Time: {search_time:.3f}s")
        logger.info(f"   ‚Ä¢ Total API Time: {api_time:.3f}s")
        logger.info(f"   ‚Ä¢ Results Returned: {len(results)}")
        
        # Analyze each result
        logger.info("üìã Individual Result Analysis:")
        for i, result in enumerate(results):
            logger.info(f"   Result {i+1}: {result.get('geo_id', 'Unknown')}")
            logger.info(f"     ‚Ä¢ Title: {result.get('title', 'N/A')[:80]}...")
            logger.info(f"     ‚Ä¢ Organism: {result.get('organism', 'N/A')}")
            logger.info(f"     ‚Ä¢ Samples: {result.get('sample_count', 'N/A')}")
            logger.info(f"     ‚Ä¢ Relevance: {result.get('relevance_score', 0):.1%}")
            logger.info(f"     ‚Ä¢ AI Insights Length: {len(str(result.get('ai_insights', '')))}")
            logger.info(f"     ‚Ä¢ Publication Date: {result.get('publication_date', 'N/A')}")
            
        # Quality assessment
        await self.assess_data_quality(query_id, query, data)
        
        # Save detailed report
        await self.save_query_report(query_id, query, data, api_time)
        
        logger.info("‚úÖ Query analysis complete!")
        logger.info("=" * 60)
    
    async def assess_data_quality(self, query_id: str, query: str, data: Dict[str, Any]):
        """Assess the quality of the returned data"""
        logger.info("üéØ STEP 3: Data quality assessment...")
        
        results = data.get("results", [])
        quality_issues = []
        quality_score = 100
        
        for i, result in enumerate(results):
            # Check for placeholder content
            ai_insights = str(result.get("ai_insights", ""))
            if "pending for recent submissions" in ai_insights.lower():
                quality_issues.append(f"Result {i+1}: AI insights appear to be placeholder")
                quality_score -= 5
            
            if result.get("publication_date") == "Recent":
                quality_issues.append(f"Result {i+1}: Publication date is generic")
                quality_score -= 2
                
            if not result.get("organism") or result.get("organism") == "Unknown":
                quality_issues.append(f"Result {i+1}: Missing organism information")
                quality_score -= 3
                
            if result.get("platform") is None:
                # This is now expected, so not counting as an issue
                pass
                
            if len(ai_insights) < 50:
                quality_issues.append(f"Result {i+1}: AI insights too short")
                quality_score -= 5
        
        logger.info(f"üìä Data Quality Score: {quality_score}/100")
        if quality_issues:
            logger.warning("‚ö†Ô∏è  Quality Issues Detected:")
            for issue in quality_issues:
                logger.warning(f"     ‚Ä¢ {issue}")
        else:
            logger.info("‚úÖ No quality issues detected!")
    
    async def save_query_report(self, query_id: str, query: str, data: Dict[str, Any], api_time: float):
        """Save detailed query report"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"query_report_{timestamp}.json"
        
        report = {
            "query_id": query_id,
            "query": query,
            "timestamp": timestamp,
            "api_time": api_time,
            "backend_search_time": data.get("search_time", 0),
            "results_count": len(data.get("results", [])),
            "total_found": data.get("total_found", 0),
            "full_response": data
        }
        
        with open(filename, "w") as f:
            json.dump(report, f, indent=2, default=str)
        
        logger.info(f"üíæ Query report saved: {filename}")

# Global monitor instance
monitor = RealTimeQueryMonitor()

async def start_monitoring():
    """Start the monitoring system"""
    logger.info("üöÄ OmicsOracle Real-Time Query Monitor")
    logger.info("=" * 50)
    logger.info("üìç Monitoring URL: http://localhost:8001")
    logger.info("üîÑ Status: Active")
    logger.info("=" * 50)
    
    # Start background monitoring
    await monitor.monitor_api_requests()

# API endpoint interceptor
async def intercept_query(query: str):
    """Function that will be called when a query is detected"""
    await monitor.track_query_execution(query)

if __name__ == "__main__":
    try:
        print("üîç Starting Real-Time Query Monitor...")
        print("üì± Now submit your query in the browser at http://localhost:8001")
        print("üîÑ Monitoring active - Press Ctrl+C to stop")
        print("=" * 60)
        
        asyncio.run(start_monitoring())
    except KeyboardInterrupt:
        print("\nüõë Monitoring stopped by user")
        monitor.monitoring = False
    except Exception as e:
        print(f"\nüí• Monitor failed: {e}")
